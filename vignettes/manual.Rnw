\documentclass{article}

\title{Network Regression for Zelig}

\author{Matt Owen, Olivia Lau, Kosuke Imai, and Gary King}

\SweaveOpts{results=hide, prefix.string=net}

\usepackage{bibentry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{url}
\usepackage{Zelig}
\usepackage{Sweave}

%\VignetteIndexEntry{Social Network Complementary Log Log Regression for Dichotomous Dependent Variables } 
%\VignettePackage{ZeligNetwork} 
%\VignetteDepends{ZeligNetwork, stats}
%\VignetteKeyWords{regression,network,zelig}

\begin{document}

\nobibliography*

<<loadLibrary, echo=F,results=hide>>= 
library(ZeligNetwork) 
@  

% CLOGLOG.NET
% CLOGLOG.NET
% CLOGLOG.NET
 
\section{{\tt cloglog.net}: Network Complementary Log Log Regression for Dichotomous Proximity Matrix Dependent Variables} 
 
Use network complementary log log regression analysis for a dependent variable that is a binary valued proximity matrix (a.k.a. sociomatricies, adjacency matrices, or matrix representations of directed graphs).  
 
\subsubsection{Syntax} 
\begin{verbatim} 
> z.out <- zelig(y ~ x1 + x2, model = "cloglog.net", data = mydata)  
> x.out <- setx(z.out) 
> s.out <- sim(z.out, x = x.out) 
\end{verbatim} 
 
 
\subsubsection{Examples} 
\begin{enumerate} 
\item Basic Example 
 
Load the sample data (see {\tt ?friendship} for details on the structure of the network dataframe): 
 
<<echo=TRUE, results=hide, fig=FALSE>>= 
data(friendship) 
@ 

Estimate model: 
 
<<echo=TRUE, results=hide, fig=FALSE>>= 
z.out <- zelig(friends ~ advice + prestige + perpower, model = "cloglog.net", data = friendship) 
summary(z.out) 
 
@ 
Setting values for the explanatory variables to their default values: 
 
<<echo=TRUE, results=hide, fig=FALSE>>= 
 x.out <- setx(z.out) 
 
@ 
Simulating quantities of interest from the posterior distribution. 
<<echo=TRUE, results=hide, fig=FALSE>>= 
s.out <- sim(z.out, x = x.out)  
summary(s.out)  
plot(s.out)  
 
@ 
 
\begin{figure}[here] 
\centering 
<<fig=TRUE, echo=false,width=12, height=6>>= 
plot(s.out) 
@ 
\label{fig:plotgam} 
\end{figure} 
 
\item Simulating First Differences 
 
Estimating the risk difference (and risk ratio) between low personal power (25th percentile) and high personal power (75th percentile) while all the other variables are held at their default values.  
 
<<echo=TRUE, results=hide, fig=FALSE>>= 
x.high <- setx(z.out, perpower = quantile(friendship$perpower, prob = 0.75))     
x.low  <- setx(z.out, perpower = quantile(friendship$perpower, prob = 0.25)) 
 
s.out2 <- sim(z.out, x = x.high, x1 = x.low)    
summary(s.out2)    
plot(s.out2)    
 
@ 
 
\begin{figure}[here] 
\centering 
<<fig=TRUE, width=12,echo=false, height=6>>= 
plot(s.out) 
@ 
\label{fig:plotgam} 
\end{figure} 
 
 
\end{enumerate} 
 
 
 
\subsubsection{Model} 
The {\tt cloglog.net} model performs a complementary log log regression of the proximity matrix $\mathbf{Y}$, a $m \times m$ matrix representing network ties, on a set of proximity matrices $\mathbf{X}$. This network regression model is directly analogous to standard complementary log log regression element-wise on the appropriately vectorized matrices. Proximity matrices are vectorized by creating $Y$, a $m^2 \times 1$ vector to represent the proximity matrix. The vectorization which produces the $Y$ vector from the $\mathbf{Y}$ matrix is performed by simple row-concatenation of $\mathbf{Y}$. For example, if $\mathbf{Y}$ is a $15 \times 15$ matrix, the $\mathbf{Y}_{1,1}$ element is the first element of $Y$, and the $\mathbf{Y}_{2,1}$ element is the second element of $Y$ and so on. Once the input matrices are vectorized, standard complementary log log regression is performed.  
 
Let $Y_{i}$ be the binary dependent variable, produced by vectorizing a binary proximity matrix, for observation $i$ which takes the value of either 0 or 1. 
\begin{itemize} 
\item The \emph{stochastic component} is given by  
\begin{equation*} 
Y_{i}  \sim \text{Bernoulli} ( \pi_{i})\\ 
\end{equation*} 
where $\pi_{i} = \text{Pr}(Y_{i} = 1)$. 
\item The \emph{systematic component} is given by: 
\begin{equation*} 
\pi_{i} = 1 - \exp[\exp (-x_i \beta)] 
\end{equation*} 
where $x_i$ the vector of $k$ explanatory variables for observation $i$ and $\beta$ is the vector of coefficients.  
\end{itemize} 
 
 
\subsubsection{Quantities of Interest} 
The quantities of interest for the network complementary log log regression are the same as those for the standard complementary log log regression.  
\begin{itemize} 
\item The expected values ({\tt qi\$ev}) for the {\tt cloglog.nett} model are simulations of the predicted probability of a success:   
\begin{equation*} 
E(Y) = \pi_{i} =  1 - \exp[\exp (-x_i \beta)], 
\end{equation*} 
given draws of $\beta$ from its sampling distribution. 
 
\item The predicted values ({\tt qi\$pr}) are draws from the Binomial distribution with mean equal to the simulated expected value $\pi_{i}$. 
 
\item The first difference ({\tt qi\$fd}) for the network complementary log log model is defined as  
\begin{equation*} 
FD = \text{Pr}(Y = 1 | x_{1}) - \text{Pr}(Y = 1| x) 
\end{equation*} 
\end{itemize} 
 
 
\subsubsection{Output Values} 
 
The output of each Zelig command contains useful information which you may view. For example, you run \verb{ z.out <- zelig(y ~ x, model = "cloglog.net", data){, then you may examine the available information in {\tt z.out} by using {\tt names(z.out)}, see the coefficients by using {\tt z.out\$coefficients}, and a default summary of information through {\tt summary(z.out)}. Other elements available through the {\tt \$} operator are listed below.  
\begin{itemize} 
\item From the {\tt zelig()} output stored in {\tt  z.out}, you may extract: 
\begin{itemize} 
\item {\tt coefficients}: parameter estimates for the explanatory variables. 
\item {\tt fitted.values}: the vector of fitted values for the explanatory variables. 
\item {\tt residuals}: the working residuals in the final iteration of the IWLS fit.  
\item {\tt linear.predictors}: the vector of $x_{i}\beta$. 
\item {\tt aic}: Akaike\'s Information Criterion (minus twice the maximized log-likelihood plus twice the number of coefficients). 
\item {\tt bic}: the Bayesian Information Criterion (minus twice the maximized log-likelihood plus the number of coefficients times log $n$). 
\item {\tt df.residual}: the residual degrees of freedom. 
\item {\tt df.null}: the residual degrees of freedom for the null model.  
\item {\tt zelig.data}: the input data frame if {\tt save.data = TRUE}  
 
\end{itemize} 
\item From {\tt summary(z.out)}(as well as from {\tt zelig()}), you may extract: 
\begin{itemize} 
\item {\tt mod.coefficients}: the parameter estimates with their associated standard errors, $p$-values, and $t$ statistics.  
\item {\tt cov.scaled}: a $k \times k$ matrix of scaled covariances. 
\item {\tt cov.unscaled}: a $k \times k$ matrix of unscaled covariances.  
\end{itemize} 
\item From the {\tt sim()} output stored in {\tt s.out}, you may extract: 
\begin{itemize} 
\item {\tt qi\$ev1}: the simulated expected probabilities for the specified values of {\tt x}. 
\item {\tt qi\$pr1}: the simulated predicted values for the specified values of {\tt x}. 
\item {\tt qi\$fd}: the simulated first differences in the expected probabilities simulated from {\tt x} and {\tt x1}. 
\end{itemize} 
\end{itemize} 


\subsection*{How to Cite Network Log-Log Regeression} 
\bibentry{ImaLauKin-cloglog.net11}

\subsection*{How to Cite the Zelig Software Package}
\CiteZelig

\subsection* {See also}
The network complementary log log regression is part of the {\tt netglm} package by Skyler J. Cranmer and is built using some of the functionality of the  {\tt sna} package by Carter T. Butts \citep{ButCar01}.In addition, advanced users may wish to refer to {\tt help(netbinom)}. Sample data are fictional.

% GAMMA.NET
% GAMMA.NET
% GAMMA.NET

\section{{\tt gamma.net}: Network Gamma Regression for Continuous, Positive Proximity Matrix Dependent Variables} 
 
Use the network gamma regression model if you have a positive-valued dependent variable that is a binary valued proximity matrix (a.k.a. sociomatricies, adjacency matrices, or matrix representations of directed graphs). The gamma distribution assumes that all waiting times are complete by the end of the study (censoring is not allowed). 
 
\subsubsection{Syntax} 
\begin{verbatim} 
> z.out <- zelig(y ~ x1 + x2, model = "gamma.net", data = mydata)  
> x.out <- setx(z.out) 
> s.out <- sim(z.out, x = x.out) 
\end{verbatim} 
 
\subsubsection{Additional Inputs} 
 
In addition to the standard inputs, {\tt zelig()} takes the following additional options for network gamma regression: 
 
\begin{itemize} 
\item {\tt LF}: specifies the link function to be used for the network gamma regression. Default is {\tt LF="inverse"}, but {\tt LF} can also be set to {\tt "identity"} or {\tt "log"} by the user. 
\end{itemize} 
 
\subsubsection{Examples} 
\begin{enumerate} 
\item Basic Example 
 
Load the sample data (see {\tt ?friendship} for details on the structure of the network dataframe): 
 
<<echo=TRUE, results=hide, fig=FALSE>>= 
data(friendship) 
 
 
 
@ 
Estimate model: 
 
<<echo=TRUE, results=hide, fig=FALSE>>= 
z.out <- zelig(per ~ perpower, LF="inverse",  model="gamma.net", data=friendship) 
summary(z.out) 
 
@ 
Setting values for the explanatory variables to their default values: 
 
<<echo=TRUE, results=hide, fig=FALSE>>= 
x.out <- setx(z.out) 
 
@ 
Simulating quantities of interest from the posterior distribution. 
<<echo=TRUE, results=hide, fig=FALSE>>= 
s.out <- sim(z.out, x = x.out)  
summary(s.out)  
plot(s.out)  
@ 
 
\begin{figure}[here] 
\centering 
<<fig=TRUE, width=12, echo=false, height=6>>= 
plot(s.out) 
@ 
\label{fig:plotgam} 
\end{figure} 
 
 
\item Simulating First Differences 
 
<<echo=TRUE, results=hide, fig=FALSE>>= 
x.low <- setx(z.out, numst2 = 0) 
x.high <- setx(z.out, numst2 = 1) 
 
s.out2 <- sim(z.out, x = x.low, x1 = x.high)    
summary(s.out2)    
plot(s.out2)    
 
@ 
 
\begin{figure}[here] 
\centering 
<<fig=TRUE, echo=false, width=12, height=6>>= 
plot(s.out) 
@ 
\label{fig:plotgam} 
\end{figure} 
 
\end{enumerate} 
 
 
 
\subsubsection{Model} 
The {\tt gamma.net} model performs a gamma regression of the proximity matrix $\mathbf{Y}$, a $m \times m$ matrix representing network ties, on a set of proximity matrices $\mathbf{X}$. This network regression model is directly analogous to standard gamma regression element-wise on the appropriately vectorized matrices. Proximity matrices are vectorized by creating $Y$, a $m^2 \times 1$ vector to represent the proximity matrix. The vectorization which produces the $Y$ vector from the $\mathbf{Y}$ matrix is performed by simple row-concatenation of $\mathbf{Y}$. For example, if $\mathbf{Y}$ is a $15 \times 15$ matrix, the $\mathbf{Y}_{1,1}$ element is the first element of $Y$, and the $\mathbf{Y}_{2,1}$ element is the second element of $Y$ and so on. Once the input matrices are vectorized, standard gamma regression is performed.  
 
Let $Y_{i}$ be the dependent variable, produced by vectorizing a binary proximity matrix, for observation $i$. 
\begin{itemize} 
\item The Gamma distribution with scale parameter $\alpha$ has a \emph{stochastic component} given by  
\begin{eqnarray*} 
Y &\sim  \text{Gamma} ( y_{i} | \lambda_i, \alpha)\\ 
f(y) & = \frac{1}{\alpha^{\lambda_i}\Gamma \lambda_i} y_i^{\lambda_i - 1} \exp - \left[ \frac{y_i}{\alpha} \right] 
\end{eqnarray*} 
for $\alpha, \lambda_i, y_i > 0$. 
\item The \emph{systematic component} is given by: 
\begin{equation*} 
\lambda_{i} =  \frac{1}{x_{i}\beta}. 
\end{equation*} 
 
\end{itemize} 
 
 
\subsubsection{Quantities of Interest} 
The quantities of interest for the network gamma regression are the same as those for the standard gamma regression.  
\begin{itemize} 
\item The expected values ({\tt qi\$ev}) are simulations of the mean of the stochastic component given draws of $\alpha$ and $\beta$ from their posteriors:    
\begin{equation*} 
E(Y) = \alpha_i \lambda. 
\end{equation*} 
 
\item The predicted values ({\tt qi\$pr}) are draws from the gamma distribution for each set of parameters $(\alpha, \lambda_i)$.  
 
\item The first difference ({\tt qi\$fd}) for the network gamma model is defined as  
\begin{equation*} 
FD = \text{Pr}(Y | x_{1}) - \text{Pr}(Y | x) 
\end{equation*} 
\end{itemize} 
 
 
\subsubsection{Output Values} 
 
The output of each Zelig command contains useful information which you may view. For example, you run \verb{ z.out <- zelig(y ~ x, model = "gamma.net", data){, then you may examine the available information in {\tt z.out} by using {\tt names(z.out)}, see the coefficients by using {\tt z.out\$coefficients}, and a default summary of information through {\tt summary(z.out)}. Other elements available through the {\tt \$} operator are listed below.  
\begin{itemize} 
\item From the {\tt zelig()} output stored in {\tt  z.out}, you may extract: 
\begin{itemize} 
\item {\tt coefficients}: parameter estimates for the explanatory variables. 
\item {\tt fitted.values}: the vector of fitted values for the explanatory variables. 
\item {\tt residuals}: the working residuals in the final iteration of the IWLS fit.  
\item {\tt linear.predictors}: the vector of $x_{i}\beta$. 
\item {\tt aic}: Akaike\'s Information Criterion (minus twice the maximized log-likelihood plus twice the number of coefficients). 
\item {\tt bic}: the Bayesian Information Criterion (minus twice the maximized log-likelihood plus the number of coefficients times log $n$). 
\item {\tt df.residual}: the residual degrees of freedom. 
\item {\tt df.null}: the residual degrees of freedom for the null model.  
\item {\tt zelig.data}: the input data frame if {\tt save.data = TRUE}  
 
\end{itemize} 
\item From {\tt summary(z.out)}(as well as from {\tt zelig()}), you may extract: 
\begin{itemize} 
\item {\tt mod.coefficients}: the parameter estimates with their associated standard errors, $p$-values, and $t$ statistics.  
\item {\tt cov.scaled}: a $k \times k$ matrix of scaled covariances. 
\item {\tt cov.unscaled}: a $k \times k$ matrix of unscaled covariances.  
\end{itemize} 
\item From the {\tt sim()} output stored in {\tt s.out}, you may extract: 
\begin{itemize} 
\item {\tt qi\$ev}: the simulated expected probabilities for the specified values of {\tt x}. 
\item {\tt qi\$pr}: the simulated predicted values drawn from a distribution defined by $(\alpha_i, \lambda)$. 
\item {\tt qi\$fd}: the simulated first differences in the expected probabilities simulated from {\tt x} and {\tt x1}. 
\end{itemize} 
\end{itemize} 
 
\subsection*{How to Cite Network Gamma Regeression} 
\bibentry{ImaLauKin-gamma.net11}

\subsection*{How to Cite the Zelig Software Package}
\CiteZelig

\subsection* {See also}
The network gamma regression is part of the {\tt netglm} package by Skyler J. Cranmer and is built using some of the functionality of the  {\tt sna} package by Carter T. Butts \citep{ButCar01}.In addition, advanced users may wish to refer to {\tt help(gamma.net)}. Sample data are fictional. 

% LOGIT.NET
% LOGIT.NET
% LOGIT.NET

\section{{\tt logit.net}: Network Logistic Regression for Dichotomous Proximity Matrix Dependent Variables}

Use network logistic regression analysis for a dependent variable that is a binary valued proximity matrix (a.k.a. sociomatricies, adjacency matrices, or matrix representations of directed graphs). 

\subsubsection{Syntax}
\begin{verbatim}
> z.out <- zelig(y ~ x1 + x2, model = "logit.net", data = mydata) 
> x.out <- setx(z.out)
> s.out <- sim(z.out, x = x.out)
\end{verbatim}



\subsubsection{Examples}
\begin{enumerate}
\item Basic Example

Load the sample data (see {\tt ?friendship} for details on the structure of the network dataframe):

<<echo=TRUE, results=hide, fig=FALSE>>=
data(friendship)



@
Estimate model:

<<echo=TRUE, results=hide, fig=FALSE>>=
z.out <- zelig(friends ~ advice + prestige + perpower, model = "logit.net", data = friendship)
summary(z.out)

@
Setting values for the explanatory variables to their default values:

<<echo=TRUE, results=hide, fig=FALSE>>=
x.out <- setx(z.out)

@
Simulating quantities of interest from the posterior distribution.
<<echo=TRUE, results=hide, fig=FALSE>>=
s.out <- sim(z.out, x = x.out) 
summary(s.out) 
plot(s.out) 

@

\begin{figure}[here]
\centering
<<fig=TRUE, echo=false,width=12, height=6>>=
plot(s.out)
@
\label{fig:plotgam}
\end{figure}


\item Simulating First Differences

Estimating the risk difference (and risk ratio) between low personal power (25th percentile) and high personal power (75th percentile) while all the other variables are held at their default values. 

<<echo=TRUE, results=hide, fig=FALSE>>=
x.high <- setx(z.out, perpower = quantile(friendship$perpower, prob = 0.75))    
x.low  <- setx(z.out, perpower = quantile(friendship$perpower, prob = 0.25))

s.out2 <- sim(z.out, x = x.high, x1 = x.low)   
summary(s.out2)   
plot(s.out2)   

@

\begin{figure}[here]
\centering
<<fig=TRUE,echo=false, width=12, height=6>>=
plot(s.out)
@
\label{fig:plotgam}
\end{figure}



\end{enumerate}



\subsubsection{Model}
The {\tt logit.net} model performs a logistic regression of the proximity matrix $\mathbf{Y}$, a $m \times m$ matrix representing network ties, on a set of proximity matrices $\mathbf{X}$. This network regression model is directly analogous to standard logistic regression element-wise on the appropriately vectorized matrices. Proximity matrices are vectorized by creating $Y$, a $m^2 \times 1$ vector to represent the proximity matrix. The vectorization which produces the $Y$ vector from the $\mathbf{Y}$ matrix is performed by simple row-concatenation of $\mathbf{Y}$. For example, if $\mathbf{Y}$ is a $15 \times 15$ matrix, the $\mathbf{Y}_{1,1}$ element is the first element of $Y$, and the $\mathbf{Y}_{2,1}$ element is the second element of $Y$ and so on. Once the input matrices are vectorized, standard logistic regression is performed. 

Let $Y_{i}$ be the binary dependent variable, produced by vectorizing a binary proximity matrix, for observation $i$ which takes the value of either 0 or 1.
\begin{itemize}
\item The \emph{stochastic component} is given by 
\begin{eqnarray*}
Y_{i} & \sim \text{Bernoulli} (y_{i} | \pi_{i})\\
& = \pi_{i}^{y_{i}} (1 - \pi_{i})^{1 - y_{i}}
\end{eqnarray*}
where $\pi_{i} = \text{Pr}(Y_{i} = 1)$.
\item The \emph{systematic component} is given by:
\begin{equation*}
\pi_{i} = \frac{1}{1 + \exp(-x_{i}\beta)}.
\end{equation*}
where $x_{i}$ is the vector of $k$ covariates for observation $i$ and $\beta$ is the vector of coefficients.
\end{itemize}


\subsubsection{Quantities of Interest}
The quantities of interest for the network logistic regression are the same as those for the standard logistic regression. 
\begin{itemize}
\item The expected values ({\tt qi\$ev}) for the {\tt logit.net} model are simulations of the predicted probability of a success:  
\begin{equation*}
E(Y) = \pi_{i} = \frac{1}{1 + \exp(-x_{i}\beta)},
\end{equation*}
given draws of $\beta$ from its sampling distribution.

\item The predicted values ({\tt qi\$pr}) are draws from the Binomial distribution with mean equal to the simulated expected value $\pi_{i}$.

\item The first difference ({\tt qi\$fd}) for the network logit model is defined as 
\begin{equation*}
FD = \text{Pr}(Y = 1 | x_{1}) - \text{Pr}(Y = 1| x)
\end{equation*}
\end{itemize}


\subsubsection{Output Values}

The output of each Zelig command contains useful information which you may view. For example, you run \verb{ z.out <- zelig(y ~ x, model = "logit.net", data){, then you may examine the available information in {\tt z.out} by using {\tt names(z.out)}, see the coefficients by using {\tt z.out\$coefficients}, and a default summary of information through {\tt summary(z.out)}. Other elements available through the {\tt \$} operator are listed below. 
\begin{itemize}
\item From the {\tt zelig()} output stored in {\tt  z.out}, you may extract:
\begin{itemize}
\item {\tt coefficients}: parameter estimates for the explanatory variables.
\item {\tt fitted.values}: the vector of fitted values for the explanatory variables.
\item {\tt residuals}: the working residuals in the final iteration of the IWLS fit. 
\item {\tt linear.predictors}: the vector of $x_{i}\beta$.
\item {\tt aic}: Akaike\'s Information Criterion (minus twice the maximized log-likelihood plus twice the number of coefficients).
\item {\tt bic}: the Bayesian Information Criterion (minus twice the maximized log-likelihood plus the number of coefficients times log $n$).
\item {\tt df.residual}: the residual degrees of freedom.
\item {\tt df.null}: the residual degrees of freedom for the null model. 
\item {\tt zelig.data}: the input data frame if {\tt save.data = TRUE} 

\end{itemize}
\item From {\tt summary(z.out)}(as well as from {\tt zelig()}), you may extract:
\begin{itemize}
\item {\tt mod.coefficients}: the parameter estimates with their associated standard errors, $p$-values, and $t$ statistics. 
\item {\tt cov.scaled}: a $k \times k$ matrix of scaled covariances.
\item {\tt cov.unscaled}: a $k \times k$ matrix of unscaled covariances. 
\end{itemize}
\item From the {\tt sim()} output stored in {\tt s.out}, you may extract:
\begin{itemize}
\item {\tt qi\$ev}: the simulated expected probabilities for the specified values of {\tt x}.
\item {\tt qi\$pr}: the simulated predicted values for the specified values of {\tt x}.
\item {\tt qi\$fd}: the simulated first differences in the expected probabilities simulated from {\tt x} and {\tt x1}.
\end{itemize}
\end{itemize}

\subsection*{How to Cite Network Log-Log Regeression} 
\bibentry{ImaLauKin-logit.net11}

\subsection*{How to Cite the Zelig Software Package}
\CiteZelig

\subsection* {See also}
The network logistic regression is part of the {\tt netglm} package by Skyler J. Cranmer and is built using some of the functionality of the  {\tt sna} package by Carter T. Butts \citep{ButCar01}.In addition, advanced users may wish to refer to {\tt help(netgamma)}. Sample data are fictional. 

% LS.NET
% LS.NET
% LS.NET

\section{{\tt ls.net}: Network Least Squares Regression for Continuous Proximity Matrix Dependent Variables}
\label{ls.net}

Use network least squares regression analysis to estimate the
best linear predictor when the dependent variable is a
continuously-valued proximity matrix (a.k.a.\ sociomatrices, adjacency
matrices, or matrix representations of directed graphs). 

\subsubsection{Syntax}
\begin{verbatim}
> z.out <- zelig(y ~  x1 + x2, model = "ls.net", data = mydata)
> x.out <- setx(z.out)
> s.out <- sim(z.out, x = x.out)
\end{verbatim}

\subsubsection{Examples}
\begin{enumerate}
\item Basic Example with First Differences 

Load sample data and format it for social networkx analysis:
%library sna required 
<<Examples.data>>=
 data(sna.ex)
@ 
<<Examples.library, echo=false>>=
if(is.na(match("sna",.packages()))){
message("Loading package sna...")
library(sna)
}
@ 
Estimate model:
<<Examples.zelig>>=
 z.out <- zelig(Var1 ~ Var2 + Var3 + Var4, model = "ls.net", data = sna.ex)

@ 

Summarize regression results:
<<Examples.summary>>=
 summary(z.out)
@ 

Set explanatory variables to their default (mean/mode) values, with
high (80th percentile) and low (20th percentile) for the second
explanatory variable (Var3).
<<Examples.setx>>=
 x.high <- setx(z.out, Var3 = quantile(sna.ex$Var3, 0.8))
 x.low <- setx(z.out, Var3 = quantile(sna.ex$Var3, 0.2))
@ 
Generate first differences for the effect of high versus low values of
Var3 on the outcome variable.
<<Examples.sim>>=
 try(s.out <- sim(z.out, x = x.high, x1 = x.low))
 try(summary(s.out))
@ 
\begin{center}
<<label=ExamplesPlot, fig=true, echo=true>>=
 plot(s.out)
@ 
\end{center}
\end{enumerate}

\subsubsection{Model}
The {\tt ls.net} model performs a least squares regression of the
sociomatrix $\mathbf{Y}$, a $m \times m$ matrix representing network
ties, on a set of sociomatrices $\mathbf{X}$. This network regression
model is a directly analogue to standard least squares regression
element-wise on the appropriately vectorized matrices. Sociomatrices
are vectorized by creating $Y$, an $m^{2} \times 1$ vector to
represent the sociomatrix. The vectorization which produces the $Y$
vector from the $\mathbf{Y}$ matrix is preformed by simple
row-concatenation of $\mathbf{Y}$. For example if $\mathbf{Y}$ is a
$15 \times 15$ matrix, the $\mathbf{Y}_{1,1}$ element is the first
element of $Y$, and the $\mathbf{Y}_{21}$ element is the second
element of $Y$ and so on. Once the input matrices are vectorized,
standard least squares regression is performed. As such:
\begin{itemize}
\item The \emph{stochastic component} is described by a density with
mean $\mu_{i}$ and the common variance $\sigma^{2}$ 
\begin{equation*}
Y_{i} \sim f(y_{i} | \mu_{i}, \sigma^{2}).
\end{equation*}

\item The \emph{systematic component} models the conditional mean as
\begin{equation*}
\mu_{i} = x_{i}\beta
\end{equation*}
where $x_{i}$ is the vector of covariates, and $\beta$ is the vector of coefficients.
\end{itemize}
The least squares estimator is the best linear predictor of a
dependent variable given $x_{i}$, and minimizes the sum of squared
errors $\sum_{i = 1}^{n} (Y_{i} - x_{i}\beta)^{2}$.

\subsubsection{Quantities of Interest}
The quantities of interest for the network least squares regression
are the same as those for the standard least squares regression.
\begin{itemize}
\item The expected value ({\tt qi\$ev}) is the mean of simulations from
the stochastic component, 
\begin{equation*}
E(Y) = x_{i}\beta,
\end{equation*}
given a draw of $\beta$ from its sampling distribution.

\item The first difference ({\tt qi\$fd}) is:
\begin{equation*}
FD = E(Y | x_{1}) - E(Y | x)
\end{equation*}
\end{itemize}

\subsubsection{Output Values}

The output of each Zelig command contains useful information which you
may view. For example, you run {\tt z.out <- zelig(y ~ x,
model="ls.net", data)}, then you may examine the available information
in {\tt z.out} by using {\tt names(z.out)}, see the coefficients by
using {\tt z.out\$coefficients}, and a default summary of information
through {\tt summary(z.out)}. Other elements available through the
{\tt \$} operator are listed below. 
\begin{itemize}
\item From the {\tt zelig()} output stored in {\tt z.out}, you may extract:
\begin{itemize}
\item {\tt coefficients}: parameter estimates for the explanatory variables.
\item {\tt fitted.values}: the vector of fitted values for the explanatory variables.
\item {\tt residuals}: the working residuals in the final iteration of the IWLS fit. 
\item {\tt df.residual}: the residual degrees of freedom.
\item {\tt zelig.data}: the input data frame if {\tt save.data = TRUE}
\end{itemize}

\item From {\tt summary(z.out)}, you may extract:
\begin{itemize}
\item {\tt mod.coefficients}: the parameter estimates with their associated standard errors, $p$-values, and $t$ statistics. 
\begin{equation*}
\hat{\beta} = \left( \sum_{i = 1}^{n} x'_{i}x_{i}  \right)^{-1} \sum x_{i}y_{i}
\end{equation*}
\item {\tt sigma}: the square root of the estimate variance of the
random error $\varepsilon$:
\begin{equation*}
\hat{\sigma} = \frac{\sum (Y_{i} - x_{i} \hat{\beta} ) ^{2}}{n - k}
\end{equation*}
\item {\tt r.squared}: the fraction of the variance explained by the model.
\begin{equation*}
R^{2} = 1 - \frac{\sum (Y_{i} - x_{i} \hat{\beta} ) ^{2}}{\sum (y_{i}
- \bar{y})^{2}}
\end{equation*}
\item {\tt adj.r.squared}: the above $R^{2}$ statistic, penalizing for
an increased number of explanatory variables.  
\item {\tt cov.unscaled}: a $k \times k$ matrix of unscaled covariances. 
\end{itemize}

\item From the {\tt sim()} output stored in {\tt s.out}, you may extract:
\begin{itemize}
\item {\tt qi\$ev}: the simulated expected values for the specified values of {\tt x}.
\item {\tt qi\$fd}: the simulated first differences (or differences in
expected values) for the specified values of {\tt x} and {\tt x1}.
\end{itemize}
\end{itemize}

\subsection* {How to Cite} 

\subsection*{How to Cite Network Linear Model}
\bibentry{ImaLauKin-ls.net11}

\subsection*{How to Cite the Zelig Software Package}
\CiteZelig

\subsection* {See also}
The network least squares regression is part of the sna package by
Carter T. Butts \citep{ButCar01}.In addition, advanced users may wish to refer to {\tt help(netlm)}.

% NORMAL.NET
% NORMAL.NET
% NORMAL.NET

\section{{\tt normal.net}: Network Normal Regression for Continuous Proximity Matrix Dependent Variables}

The Network Normal regression model is a close variant of the more standard least squares regression model (see {\tt netlm}). Both models specify a continuous proximity matrix (a.k.a. sociomatricies, adjacency matrices, or matrix representations of directed graphs) dependent variable as a linear function of a set of explanatory variables. The network Normal model reports maximum likelihood (rather than least squares) estimates. The two models differ only in their estimate for the stochastic parameter $\sigma$.

\subsubsection{Syntax}
\begin{verbatim}
> z.out <- zelig(y ~ x1 + x2, model = "normal.net", data = mydata) 
> x.out <- setx(z.out)
> s.out <- sim(z.out, x = x.out)
\end{verbatim}

\subsubsection{Additional Inputs}

In addition to the standard inputs, {\tt zelig()} takes the following additional options for network normal regression:

\begin{itemize}
\item {\tt LF}: specifies the link function to be used for the network normal regression. Default is {\tt LF="identity"}, but {\tt LF} can also be set to {\tt "log"} or {\tt "inverse"} by the user.
\end{itemize}

\subsubsection{Examples}
\begin{enumerate}
\item Basic Example

Load the sample data (see {\tt ?friendship} for details on the structure of the network dataframe):

<<echo=TRUE, results=hide, fig=FALSE>>=
data(friendship)



@
Estimate model:

<<echo=TRUE, results=hide, fig=FALSE>>=
z.out <- zelig(perpower ~ friends + advice + prestige, model = "normal.net", data = friendship)
summary(z.out)

@
Setting values for the explanatory variables to their default values:

<<echo=TRUE, results=hide, fig=FALSE>>=
x.out <- setx(z.out)

@
Simulate fitted values.
<<echo=TRUE, results=hide, fig=FALSE>>=
s.out <- sim(z.out, x = x.out) 
summary(s.out) 
plot(s.out) 

@

\begin{figure}[here]
\centering
<<fig=TRUE,echo=false, width=12, height=6>>=
plot(s.out)
@
\label{fig:plotgam}
\end{figure}

\end{enumerate}



\subsubsection{Model}
The {\tt normal.net} model performs a Normal regression of the proximity matrix $\mathbf{Y}$, a $m \times m$ matrix representing network ties, on a set of proximity matrices $\mathbf{X}$. This network regression model is directly analogous to standard Normal regression element-wise on the appropriately vectorized matrices. Proximity matrices are vectorized by creating $Y$, a $m^2 \times 1$ vector to represent the proximity matrix. The vectorization which produces the $Y$ vector from the $\mathbf{Y}$ matrix is performed by simple row-concatenation of $\mathbf{Y}$. For example, if $\mathbf{Y}$ is a $15 \times 15$ matrix, the $\mathbf{Y}_{1,1}$ element is the first element of $Y$, and the $\mathbf{Y}_{2,1}$ element is the second element of $Y$ and so on. Once the input matrices are vectorized, standard Normal regression is performed. 

Let $Y_{i}$ be the continuous dependent variable, produced by vectorizing a continuous proximity matrix, for observation $i$.  
\begin{itemize}
\item The \emph{stochastic component} is described by a univariate normal model with a vector of means $\mu_i$ and scalar variance $\sigma^2$: 
\begin{equation*}
Y_{i}  \sim \text{Normal} ( \mu_{i}, \sigma^2).\\
\end{equation*}
\item The \emph{systematic component} is given by:
\begin{equation*}
\mu_i = x_i\beta.
\end{equation*}
where $ x_i$ is the vector of $k$ explanatory variables and $\beta$ is the vector of coefficients.
\end{itemize}


\subsubsection{Quantities of Interest}
The quantities of interest for the network Normal regression are the same as those for the standard Normal regression. 
\begin{itemize}
\item The expected value ({\tt qi\$ev}) for the {\tt normal.net} model is the mean of simulations from the stochastic component,  
\begin{equation*}
E(Y) = \mu_{i} =  x_{i}\beta,
\end{equation*}
given a draw of $\beta$ from its posterior.

\item The predicted value ({\tt qi\$pr}) is a draw from the distribution defined by the set of parameters $(\mu_i, \sigma^2)$. 

\item The first difference ({\tt qi\$fd}) for the network Normal model is defined as 
\begin{equation*}
FD = \text{Pr}(Y| x_{1}) - \text{Pr}(Y| x)
\end{equation*}
\end{itemize}


\subsubsection{Output Values}

The output of each Zelig command contains useful information which you may view. For example, you run \verb{ z.out <- zelig(y ~ x, model = "normal.net", data){, then you may examine the available information in {\tt z.out} by using {\tt names(z.out)}, see the coefficients by using {\tt z.out\$coefficients}, and a default summary of information through {\tt summary(z.out)}. Other elements available through the {\tt \$} operator are listed below. 
\begin{itemize}
\item From the {\tt zelig()} output stored in {\tt  z.out}, you may extract:
\begin{itemize}
\item {\tt coefficients}: parameter estimates for the explanatory variables.
\item {\tt fitted.values}: the vector of fitted values for the systemic component $\lambda$.
\item {\tt residuals}: the working residuals in the final iteration of the IWLS fit. 
\item {\tt linear.predictors}: fitted values. For the normal model, these are identical to fitted values.
\item {\tt aic}: Akaike's Information Criterion (minus twice the maximized log-likelihood plus twice the number of coefficients).
\item {\tt bic}: the Bayesian Information Criterion (minus twice the maximized log-likelihood plus the number of coefficients times log $n$).
\item {\tt df.residual}: the residual degrees of freedom.
\item {\tt df.null}: the residual degrees of freedom for the null model. 
\item {\tt zelig.data}: the input data frame if {\tt save.data = TRUE} 

\end{itemize}
\item From {\tt summary(z.out)}(as well as from {\tt zelig()}), you may extract:
\begin{itemize}
\item {\tt mod.coefficients}: the parameter estimates with their associated standard errors, $p$-values, and $t$ statistics. 
\item {\tt cov.scaled}: a $k \times k$ matrix of scaled covariances.
\item {\tt cov.unscaled}: a $k \times k$ matrix of unscaled covariances. 
\end{itemize}
\item From the {\tt sim()} output stored in {\tt s.out}, you may extract:
\begin{itemize}
\item {\tt qi\$ev}: the simulated expected probabilities for the specified values of {\tt x}.
\item {\tt qi\$pr}: the simulated predicted values drawn from the distribution defined by $(\mu_i, \sigma^2)$.
\item {\tt qi\$fd}: the simulated first differences in the expected probabilities simulated from {\tt x} and {\tt x1}.
\end{itemize}
\end{itemize}

\subsection*{How to Cite Network Normal Regression}
\bibentry{ImaLauKin-normal.net11}

\subsection*{How to Cite the Zelig Software Package}
\CiteZelig

\subsection* {See also}
The network normal regression is part of the {\tt netglm} package by Skyler J. Cranmer and is built using some of the functionality of the  {\tt sna} package by Carter T. Butts \citep{ButCar01}.In addition, advanced users may wish to refer to {\tt help(normal.net)}. Sample data are fictional. 

% POISSON.NET
% POISSON.NET
% POISSON.NET

\section{{\tt poisson.net}: Network Poisson Regression for Count Proximity 
Matrix Dependent Variables
}\label{poisson.net}

Use the ordinal logit regression model if your dependent variable is
ordered and categorical, either in the form of integer values or character strings.  

\subsubsection{Syntax}

\begin{verbatim}
> z.out <- zelig(as.factor(Y) ~ X1 + X2, model = "poisson.net", data = mydata)
> x.out <- setx(z.out)
> s.out <- sim(z.out, x = x.out)
\end{verbatim}
If {\tt Y} takes discrete integer values, the {\tt as.factor()}
command will order automatically order the values.  If {\tt Y} takes
on values composed of character strings, such as ``strongly agree'',
``agree'', and ``disagree'', {\tt as.factor()} will order the values
in the order in which they appear in {\tt Y}.  You will need to
replace your dependent variable with a factored variable prior to
estimating the model through {\tt zelig()}.  See Section \ref{factors}
for more information on creating ordered factors and Example
\ref{ord.fact} below.

\subsubsection{Example}

\begin{enumerate}

\item {Creating An Ordered Dependent Variable} \label{ord.fact}

Load the sample data:  
<<Example.data>>=
 data(sanction)
@ 
Create an ordered dependent variable: 
<<Example.factor>>=
 sanction$ncost <- factor(sanction$ncost, ordered = TRUE,
                          levels = c("net gain", "little effect", 
                          "modest loss", "major loss"))
@ 
Estimate the model:
<<Example.zelig>>=
  # z.out <- zelig(ncost ~ mil + coop, model = "poisson.net", data = sanction)
@ 
Set the explanatory variables to their observed values:  
<<Example.setx>>=
  # x.out <- setx(z.out, fn = NULL)
@ 
Simulate fitted values given {\tt x.out} and view the results:
<<Example.sim>>=
  # s.out <- sim(z.out, x = x.out)
@
<<Example.summary>>= 
  # summary(s.out)
@ 

\item {First Differences}

Using the sample data \texttt{sanction}, estimate the empirical model and returning the coefficients:
<<FirstDifferences.zelig>>=
  # z.out <- zelig(as.factor(cost) ~ mil + coop, model = "ologit", 
  #                data = sanction)
@ 
<<FirstDifferences.summary>>=
  # summary(z.out)
@ 
Set the explanatory variables to their means, with {\tt mil} set
to 0 (no military action in addition to sanctions) in the baseline
case and set to 1 (military action in addition to sanctions) in the
alternative case:
<<FirstDifferences.setx>>=
  # x.low <- setx(z.out, mil = 0)
  # x.high <- setx(z.out, mil = 1)
@ 
Generate simulated fitted values and first differences, and view the results:
<<FirstDifferences.sim>>=
  # s.out <- sim(z.out, x = x.low, x1 = x.high)
  # summary(s.out)
@ 
\end{enumerate}

\subsubsection{Model}

Let $Y_i$ be the ordered categorical dependent variable for
observation $i$ that takes one of the integer values from $1$ to $J$
where $J$ is the total number of categories.
  
\begin{itemize}
\item The \emph{stochastic component} begins with an unobserved continuous
  variable, $Y^*_i$, which follows the standard logistic distribution
  with a parameter $\mu_i$,
  \begin{equation*}
    Y_i^* \; \sim \; \textrm{Logit}(y_i^* \mid \mu_i),  
  \end{equation*}
  to which we add an observation mechanism
  \begin{equation*}
    Y_i \; = \; j \quad {\rm if} \quad \tau_{j-1} \le Y_i^* \le \tau_j
    \quad {\rm for} \quad j=1,\dots,J.
  \end{equation*}
  where $\tau_l$ (for $l=0,\dots,J$) are the threshold parameters with
  $\tau_l < \tau_m$ for all $l<m$ and $\tau_0=-\infty$ and
  $\tau_J=\infty$.
  
\item The \emph{systematic component} has the following form, given
  the parameters $\tau_j$ and $\beta$, and the explanatory variables $x_i$: 
  \begin{equation*}
    \Pr(Y \le j) \; = \; \Pr(Y^* \le \tau_j) \; = \frac{\exp(\tau_j -
      x_i \beta)}{1+\exp(\tau_j -x_i \beta)},
  \end{equation*}
  which implies:
  \begin{equation*}
    \pi_{j}  \; = \; \frac{\exp(\tau_j - x_i \beta)}{1 + \exp(\tau_j -
      x_i \beta)} - \frac{\exp(\tau_{j-1} - x_i \beta)}{1 +
      \exp(\tau_{j-1} - x_i \beta)}.
  \end{equation*}
\end{itemize}

\subsubsection{Quantities of Interest} 

\begin{itemize}
\item The expected values ({\tt qi\$ev}) for the ordinal logit model
  are simulations of the predicted probabilities for each category: 
\begin{equation*}
E(Y = j) \; = \; \pi_{j} \; = \; \frac{\exp(\tau_j - x_i \beta)}
{1 + \exp(\tau_j - x_i \beta)} - \frac{\exp(\tau_{j-1} - x_i \beta)}{1 +
 \exp(\tau_{j-1} - x_i \beta)},
\end{equation*}
given a draw of $\beta$ from its sampling distribution.  

\item The predicted value ({\tt qi\$pr}) is drawn from the logit
  distribution described by $\mu_i$, and observed as one of $J$
  discrete outcomes.  

\item The difference in each of the predicted probabilities ({\tt
    qi\$fd}) is given by
  \begin{equation*}
    \Pr(Y=j \mid x_1) \;-\; \Pr(Y=j \mid x) \quad {\rm for} \quad
    j=1,\dots,J.
  \end{equation*}

\item In conditional prediction models, the average expected treatment
  effect ({\tt att.ev}) for the treatment group is 
    \begin{equation*} \frac{1}{n_j}\sum_{i:t_i=1}^{n_j} \left\{ Y_i(t_i=1) -
      E[Y_i(t_i=0)] \right\},
    \end{equation*} 
where $t_{i}$ is a binary explanatory variable defining the treatment
($t_{i}=1$) and control ($t_{i}=0$) groups, and $n_j$ is the 
number of treated observations in category $j$.

\item In conditional prediction models, the average predicted treatment
  effect ({\tt att.pr}) for the treatment group is 
    \begin{equation*} \frac{1}{n_j}\sum_{i:t_i=1}^{n_j} \left\{ Y_i(t_i=1) -
      \widehat{Y_i(t_i=0)} \right\},
    \end{equation*} 
where $t_{i}$ is a binary explanatory variable defining the treatment
($t_{i}=1$) and control ($t_{i}=0$) groups, and $n_j$ is the 
number of treated observations in category $j$.

\end{itemize}

\subsubsection{Output Values}

The output of each Zelig command contains useful information which you
may view.  For example, if you run \texttt{z.out <- zelig(y \~\,
  x, model = "ologit", data)}, then you may examine the available
information in \texttt{z.out} by using \texttt{names(z.out)},
see the {\tt coefficients} by using {\tt z.out\$coefficients}, and
a default summary of information through \texttt{summary(z.out)}.
Other elements available through the {\tt \$} operator are listed
below.

\begin{itemize}
\item From the {\tt zelig()} output object {\tt z.out}, you may
  extract:
   \begin{itemize}
   \item {\tt coefficients}: parameter estimates for the explanatory
     variables.
   \item {\tt zeta}: a vector containing the estimated class
     boundaries $\tau_j$.
   \item {\tt deviance}: the residual deviance.
   \item {\tt fitted.values}: the $n \times J$ matrix of in-sample
     fitted values.
   \item {\tt df.residual}: the residual degrees of freedom.
   \item {\tt edf}: the effective degrees of freedom.  
   \item {\tt Hessian}: the Hessian matrix.
   \item {\tt zelig.data}: the input data frame if {\tt save.data = TRUE}.  
   \end{itemize}

\item From {\tt summary(z.out)}, you may extract: 
   \begin{itemize}
   \item {\tt coefficients}: the parameter estimates with their
     associated standard errors, and $t$-statistics.
   \end{itemize}
   
 \item From the {\tt sim()} output object {\tt s.out}, you may extract
   quantities of interest arranged as arrays.  Available quantities
   are:

   \begin{itemize}
   \item {\tt qi\$ev}: the simulated expected probabilities for the
     specified values of {\tt x}, indexed by simulation $\times$
     quantity $\times$ {\tt x}-observation (for more than one {\tt
       x}-observation).
   \item {\tt qi\$pr}: the simulated predicted values drawn from the
     distribution defined by the expected probabilities, indexed by
     simulation $\times$ {\tt x}-observation.
   \item {\tt qi\$fd}: the simulated first difference in the predicted
     probabilities for the values specified in {\tt x} and {\tt x1},
     indexed by simulation $\times$ quantity $\times$ {\tt
       x}-observation (for more than one {\tt x}-observation).
   \item {\tt qi\$att.ev}: the simulated average expected treatment
     effect for the treated from conditional prediction models.  
   \item {\tt qi\$att.pr}: the simulated average predicted treatment
     effect for the treated from conditional prediction models.  
   \end{itemize}
\end{itemize}

\subsection*{How to Cite Network Poisson Regression}
\bibentry{ImaLauKin-poisson.net11}

\subsection*{How to Cite the Zelig Software Package}
\CiteZelig

\subsection* {See also}
The network poisson regression is part of the {\tt netglm} package by Skyler J. Cranmer and is built using some of the functionality of the  {\tt sna} package by Carter T. Butts \citep{ButCar01}.In addition, advanced users may wish to refer to {\tt help(poisson.net)}. Sample data are fictional. 

% PROBIT.NET
% PROBIT.NET
% PROBIT.NET

\section{{\tt probit.net}: Network Probit Regression for Dichotomous Proximity Matrix Dependent Variables}

Use network probit regression analysis for a dependent variable that is a binary valued proximity matrix (a.k.a. sociomatricies, adjacency matrices, or matrix representations of directed graphs). 

\subsubsection{Syntax}
\begin{verbatim}
> z.out <- zelig(y ~ x1 + x2, model = "probit.net", data = mydata) 
> x.out <- setx(z.out)
> s.out <- sim(z.out, x = x.out)
\end{verbatim}



\subsubsection{Examples}
\begin{enumerate}
\item Basic Example

Load the sample data (see {\tt ?friendship} for details on the structure of the network dataframe):

<<echo=TRUE, results=hide, fig=FALSE>>=
data(friendship)


@
Estimate model:

<<echo=TRUE, results=hide, fig=FALSE>>=
z.out <- zelig(friends ~ advice + prestige + perpower, model = "probit.net", data = friendship)
summary(z.out)

@
Setting values for the explanatory variables to their default values:

<<echo=TRUE, results=hide, fig=FALSE>>=
x.out <- setx(z.out)

@
Simulating quantities of interest from the posterior distribution.
<<echo=TRUE, results=hide, fig=FALSE>>=
s.out <- sim(z.out, x = x.out) 
summary(s.out) 
plot(s.out) 

@

\begin{figure}[here]
\centering
<<fig=TRUE,echo=false, width=6, height=6>>=
plot(s.out)
@
\label{fig:plotgam}
\end{figure}

@
\item Simulating First Differences

Estimating the risk difference (and risk ratio) between low personal power (25th percentile) and high personal power (75th percentile) while all the other variables are held at their default values. 

<<echo=TRUE, results=hide, fig=FALSE>>=
x.high <- setx(z.out, perpower = quantile(friendship$perpower, prob = 0.75))    
x.low  <- setx(z.out, perpower = quantile(friendship$perpower, prob = 0.25))

s.out2 <- sim(z.out, x = x.high, x1 = x.low)   
summary(s.out2)   
plot(s.out2)   

@

\begin{figure}[here]
\centering
<<fig=TRUE,echo=false, width=6, height=6>>=
plot(s.out)
@
\label{fig:plotgam}
\end{figure}

\end{enumerate}



\subsubsection{Model}
The {\tt probit.net} model performs a probit regression of the proximity matrix $\mathbf{Y}$, a $m \times m$ matrix representing network ties, on a set of proximity matrices $\mathbf{X}$. This network regression model is directly analogous to standard probit regression element-wise on the appropriately vectorized matrices. Proximity matrices are vectorized by creating $Y$, a $m^2 \times 1$ vector to represent the proximity matrix. The vectorization which produces the $Y$ vector from the $\mathbf{Y}$ matrix is performed by simple row-concatenation of $\mathbf{Y}$. For example, if $\mathbf{Y}$ is a $15 \times 15$ matrix, the $\mathbf{Y}_{1,1}$ element is the first element of $Y$, and the $\mathbf{Y}_{2,1}$ element is the second element of $Y$ and so on. Once the input matrices are vectorized, standard probit regression is performed. 

Let $Y_{i}$ be the binary dependent variable, produced by vectorizing a binary proximity matrix, for observation $i$ which takes the value of either 0 or 1.
\begin{itemize}
\item The \emph{stochastic component} is given by 
\begin{equation*}
Y_{i}  \sim \text{Bernoulli} ( \pi_{i})\\
\end{equation*}
where $\pi_{i} = \text{Pr}(Y_{i} = 1)$.
\item The \emph{systematic component} is given by:
\begin{equation*}
\pi_{i} = \pmb{\Phi}(x_{i}\beta).
\end{equation*}
where $ \pmb{\Phi}(\mu)$ is the cumulative distribution function of the Normal distribution with mean 0 and unit variance. 
\end{itemize}


\subsubsection{Quantities of Interest}
The quantities of interest for the network probit regression are the same as those for the standard probit regression. 
\begin{itemize}
\item The expected values ({\tt qi\$ev}) for the {\tt probit.net} model are simulations of the predicted probability of a success:  
\begin{equation*}
E(Y) = \pi_{i} =  \pmb{\Phi}(x_{i}\beta),
\end{equation*}
given draws of $\beta$ from its sampling distribution.

\item The predicted values ({\tt qi\$pr}) are draws from the Binomial distribution with mean equal to the simulated expected value $\pi_{i}$.

\item The first difference ({\tt qi\$fd}) for the network probit model is defined as 
\begin{equation*}
FD = \text{Pr}(Y = 1 | x_{1}) - \text{Pr}(Y = 1| x)
\end{equation*}
\end{itemize}


\subsubsection{Output Values}

The output of each Zelig command contains useful information which you may view. For example, you run \verb{ z.out <- zelig(y ~ x, model = "probit.net", data){, then you may examine the available information in {\tt z.out} by using {\tt names(z.out)}, see the coefficients by using {\tt z.out\$coefficients}, and a default summary of information through {\tt summary(z.out)}. Other elements available through the {\tt \$} operator are listed below. 
\begin{itemize}
\item From the {\tt zelig()} output stored in {\tt  z.out}, you may extract:
\begin{itemize}
\item {\tt coefficients}: parameter estimates for the explanatory variables.
\item {\tt fitted.values}: the vector of fitted values for the explanatory variables.
\item {\tt residuals}: the working residuals in the final iteration of the IWLS fit. 
\item {\tt linear.predictors}: the vector of $x_{i}\beta$.
\item {\tt aic}: Akaike\'s Information Criterion (minus twice the maximized log-likelihood plus twice the number of coefficients).
\item {\tt bic}: the Bayesian Information Criterion (minus twice the maximized log-likelihood plus the number of coefficients times log $n$).
\item {\tt df.residual}: the residual degrees of freedom.
\item {\tt df.null}: the residual degrees of freedom for the null model. 
\item {\tt zelig.data}: the input data frame if {\tt save.data = TRUE} 

\end{itemize}
\item From {\tt summary(z.out)}(as well as from {\tt zelig()}), you may extract:
\begin{itemize}
\item {\tt mod.coefficients}: the parameter estimates with their associated standard errors, $p$-values, and $t$ statistics. 
\item {\tt cov.scaled}: a $k \times k$ matrix of scaled covariances.
\item {\tt cov.unscaled}: a $k \times k$ matrix of unscaled covariances. 
\end{itemize}
\item From the {\tt sim()} output stored in {\tt s.out}, you may extract:
\begin{itemize}
\item {\tt qi\$ev}: the simulated expected probabilities for the specified values of {\tt x}.
\item {\tt qi\$pr}: the simulated predicted values for the specified values of {\tt x}.
\item {\tt qi\$fd}: the simulated first differences in the expected probabilities simulated from {\tt x} and {\tt x1}.
\end{itemize}
\end{itemize}

\subsection*{How to Cite Network Poisson Regression}
\bibentry{ImaLauKin-poisson.net11}

\subsection*{How to Cite the Zelig Software Package}
\CiteZelig

\subsection* {See also}
The network probit regression is part of the {\tt netglm} package by Skyler J. Cranmer and is built using some of the functionality of the  {\tt sna} package by Carter T. Butts \citep{ButCar01}.In addition, advanced users may wish to refer to {\tt help(netpoisson)}. Sample data are fictional. 

\bibliographystyle{plain}
\bibliography{gk,gkpubs,ZeligNetwork}
 
\end{document}

